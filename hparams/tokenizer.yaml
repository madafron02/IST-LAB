# ############################################################################
# Tokenizer: subword BPE tokenizer with unigram 5K
# Training: Mini-LibriSpeech
# Authors:  Abdel Heba 2021
#           Mirco Ravanelli 2021
# ############################################################################


# Set up folders for reading from and writing to
csv_folder: /scratch/mfron/IST-LAB/data
data_folder: /scratch/mfron/IST-LAB/JASMIN/kaldi_processed
output_folder: /scratch/mfron/IST-LAB/save

# Path where data-specification files are stored
train_splits: ["train-clean-100", "train-clean-360", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
skip_prep: False
train_csv: !ref <csv_folder>/train.csv
valid_csv: !ref <csv_folder>/val.csv

# Tokenizer parameters
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 1000  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
# annotation_read: transcription # field to read
csv_read: transcription
bos_id: 1
eos_id: 2

# Tokenizer object
tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   bos_id: !ref <bos_id>
   eos_id: !ref <eos_id>
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train_csv>
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type> # ["unigram", "bpe", "char"]
   character_coverage: !ref <character_coverage>
   annotation_list_to_check: [!ref <train_csv>, !ref <valid_csv>]
   # model_dir: !ref <output_folder>
   # vocab_size: !ref <token_output>
   # annotation_train: !ref <train_annotation>
   # annotation_read: !ref <annotation_read>
   # model_type: !ref <token_type> # ["unigram", "bpe", "char"]
   # character_coverage: !ref <character_coverage>
   # annotation_list_to_check: [!ref <train_annotation>, !ref <valid_annotation>]
   # annotation_format: json
